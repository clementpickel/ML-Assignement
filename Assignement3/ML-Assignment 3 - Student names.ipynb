{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca31d406",
   "metadata": {},
   "source": [
    "# Machine Learning - Assignment 3\n",
    "\n",
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531893a",
   "metadata": {},
   "source": [
    "The aim of the assignment is to implement an artificial neural network (mostly) from scratch. This includes implementing or fixing the following:\n",
    "\n",
    "* Add support for additional activation functions and their derivatives.\n",
    "* Add support for loss functions and their derivative.\n",
    "* Add the use of a bias in the forward propagation.\n",
    "* Add the use of a bias in the backward propagation.\n",
    "\n",
    "In addition, you will we doing the following as well:\n",
    "\n",
    "* Test the algorithm on 3 datasets.\n",
    "* Compare neural networks with and without scaling.\n",
    "* Hyper-parameter tuning.\n",
    "\n",
    "The forward and backward propagation is made to work through a single layer, and are re-used multiple times to work for multiple layers.\n",
    "\n",
    "Follow the instructions and implement what is missing to complete the assignment. Some functions have been started to help you a little bit with the implementation.\n",
    "\n",
    "**Note:** You might need to go back and forth during your implementation of the code. The structure is set up to make implementation easier, you might find yourself going back and and forth to change something to make it easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb200d9",
   "metadata": {},
   "source": [
    "## Assignment preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a64bd",
   "metadata": {},
   "source": [
    "We help you out with importing the libraries.\n",
    "\n",
    "**IMPORTANT NOTE:** You may not import any more libraries than the ones already imported!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0711fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We set seed to better reproduce results later on.\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1242d5e",
   "metadata": {},
   "source": [
    "## Neural Network utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0c6ac",
   "metadata": {},
   "source": [
    "### 1) Activation functions\n",
    "\n",
    "Below is some setup for choosing activation function. Implement 2 additional activation functions, \"ReLU\" and one more of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca10df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def activate(activations, selected_function = \"none\"):\n",
    "    \n",
    "    if selected_function == \"none\":\n",
    "        y = activations\n",
    "    elif selected_function == \"relu\" :\n",
    "        # TODO: Implement the \"ReLU\" activation function\n",
    "        y = np.maximum(0, activations)\n",
    "    elif selected_function == \"sigmoid\" :\n",
    "        # TODO: Implement another activation function activation function of your own choice.\n",
    "        y = 1 / (1 + np.exp(-activations))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ebbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Test your activation functions, is the returning values what you expect?\n",
    "test_values = np.array([-2, -1, 0, 1, 2, 3, 5])\n",
    "print(\"Activation 'none' :\", activate(test_values, \"none\"))\n",
    "print(\"Activation 'relu' :\", activate(test_values, \"relu\"))\n",
    "print(\"Activation 'sigmoid' :\", activate(test_values, \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7f8fa",
   "metadata": {},
   "source": [
    "### 2) Activation function derivatives\n",
    "\n",
    "Neural networks need both the activation function and its derivative. Finish the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17c00420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_activate(activations, selected_function = \"none\"):\n",
    "    if selected_function == \"none\":\n",
    "        dy = np.ones_like(activations)\n",
    "    elif selected_function == \"relu\":\n",
    "        # TODO: Implement the \"ReLU\" derivative\n",
    "        dy = np.where(activations > 0, 1, 0)\n",
    "    elif selected_function == \"sigmoid\" :\n",
    "        # TODO: Implement the derivative of the activation function you chose yourself.\n",
    "        sigmoid = 1 / (1 + np.exp(-activations))\n",
    "        dy = sigmoid * (1 - sigmoid)\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea074e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Test your activation function derivatives, is the returning values what you expect?\n",
    "test_values = np.array([-2, -1, 0, 1, 2, 3, 5])\n",
    "print(\"Derivative 'none' :\", d_activate(test_values, \"none\"))\n",
    "print(\"Derivative 'relu' :\", d_activate(test_values, \"relu\"))\n",
    "print(\"Derivative 'sigmoid' :\", d_activate(test_values, \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f72d0",
   "metadata": {},
   "source": [
    "### 3) Loss functions\n",
    "\n",
    "To penalize the network when it predicts incorrect, we need to meassure how \"bad\" the prediction is. This is done with loss-functions.\n",
    "\n",
    "Similar as with the activation functions, the loss function needs its derivative as well.\n",
    "\n",
    "Finish the MSE_loss (Mean Squared Error loss), as well as adding one additional loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ff5b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the loss for a set of predictions y_hat compared to a set of real valyes y\n",
    "def MSE_loss(y_hat, y):\n",
    "    loss = np.mean((y_hat - y) ** 2) # TODO: Finish this function\n",
    "    return loss\n",
    "\n",
    "\n",
    "# TODO: Choose another loss function and implement it\n",
    "def BCE_loss(y_hat, y):\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) # TODO: Finish this function\n",
    "    return loss\n",
    "\n",
    "\n",
    "def CCE_loss(y_hat, y):\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(np.sum(y * np.log(y_hat), axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965be68",
   "metadata": {},
   "source": [
    "The derivatives of the loss is with respect to the predicted value **y_hat**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dea0d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_MSE_loss(y_hat, y):\n",
    "    dy = 2 * (y_hat - y) / y.size \n",
    "    # TODO: Finish this function\n",
    "    return dy\n",
    "\n",
    "# TODO: Choose another loss function and implement it\n",
    "def d_BCE_loss(y_hat, y):\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    dy = (y_hat - y) / (y_hat * (1 - y_hat) * y.size)\n",
    "    # TODO: Finish this function\n",
    "    return dy\n",
    "\n",
    "\n",
    "def d_CCE_loss(y_hat, y):\n",
    "    dy = (y_hat - y) / y.shape[0]\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfeb3b",
   "metadata": {},
   "source": [
    "### 4) Forward propagation\n",
    "\n",
    "The first \"fundamental\" function for neural networks is to be able to propagate the data forward through the neural network. We will implement this function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32b521f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_forward(weights, activations, bias, activation_function=\"none\"):\n",
    "    \n",
    "    # TODO: Add support for the use of bias\n",
    "\n",
    "    dot_product = np.dot(weights, activations) + bias\n",
    "\n",
    "    new_activations = activate(dot_product, activation_function)\n",
    "\n",
    "    return new_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4abe8c",
   "metadata": {},
   "source": [
    "### 5) Back-propagation\n",
    "\n",
    "To be able to train a neural network, we need to be able to propagate the loss backwards and update the weights. We will implement this function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83b2ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the backward gradients that are passed throught the layer in the backward pass.\n",
    "# Returns both the derivative of the loss in respect to the weights and the input signal (activations).\n",
    "\n",
    "def propagate_backward(weights, activations, dl_dz, bias, activation_function=\"none\"):\n",
    "    # NOTE: dl_dz is the derivative of the loss based on the previous layers activations/outputs\n",
    "\n",
    "    # TODO: Add support for the use of bias\n",
    "\n",
    "    z = np.dot(weights, activations) + bias\n",
    "    d_loss = d_activate(z, activation_function) * dl_dz\n",
    "\n",
    "    d_weights = np.dot(d_loss, activations.T)\n",
    "    \n",
    "    d_bias = np.sum(d_loss, axis=1, keepdims=True)\n",
    "    d_activations = np.dot(weights.T, d_loss)\n",
    "    \n",
    "    return d_weights, d_activations, d_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73645779",
   "metadata": {},
   "source": [
    "## Neural network implementation\n",
    "\n",
    "### 6) Fixing the neural network\n",
    "\n",
    "Below is a class implementation of a MLP neural network. This implementation is still lacking several areas that are needed for the network to be robust and function well. Your task is to improve and fix it with the following:\n",
    "\n",
    "1. Add a bias to the activation functions, and make sure the bias is also updated during training. \n",
    "2. Add a function that trains the network using minibatches (such that the neural network trains on a few samples at a time). \n",
    "3. Make use of an validation set in the training function. The model should stop training when the loss starts to increase for the validatin set. This feature should be able to be turned on and off to test the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c550aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \n",
    "    # Setup all parameters and activation functions.\n",
    "    # This function runs directly when a new instance of this class is created.\n",
    "    def __init__ (self, input_dim, output_dim, neurons = []):\n",
    "\n",
    "        # NOTE: The \"neurons\" parameter is given as a list.\n",
    "        # E.g., [4, 8, 4] means 4 neurons in layer 1, 8 neurons in layer 2 etc...\n",
    "\n",
    "        # TODO: Add support for bias for each neuron in the code below.\n",
    "        \n",
    "        self.weights = [np.random.normal(0, 2, (m, n)) for n, m in zip([input_dim] + neurons, neurons + [output_dim])]\n",
    "\n",
    "        \n",
    "        self.biases = [np.zeros((layer_size, 1)) for layer_size in neurons + [output_dim]]\n",
    "        \n",
    "        self.activation_functions = [\"relu\"] * len(neurons) + [\"none\"]\n",
    "\n",
    "    # Predict the input throught the network and calculate the output.\n",
    "    def forward(self, x):\n",
    "        activation = x.T\n",
    "        # TODO: Add support for a bias for each neuron in the code below.\n",
    "        for layer_weights, layer_bias, layer_activation_function in zip(self.weights,self.biases , self.activation_functions):\n",
    "            activation = propagate_forward(layer_weights, activation, layer_bias, layer_activation_function)\n",
    "            \n",
    "        return activation.T\n",
    "    \n",
    "    \n",
    "    # Adjust the weights in the network to better fit the desired output (y), given the input (x).\n",
    "    # The weight updates are happening \"in-place\", thus we are only returning the loss from this function.\n",
    "    # Note that this function can handle a variable size of the input (x), both full datasets or smaller parts of the dataset.\n",
    "    def adjust_weights(self, x, y, learning_rate=1e-4):\n",
    "                \n",
    "        # TODO: Add support for a bias for each neuron and make sure these are learnt as well in the code below.\n",
    "\n",
    "        activation = x.T\n",
    "        activation_history = [activation] # NOTE: We need the previous (or intermediate) activations to make use of the \"chain rule\" (see lecture notes).\n",
    "        \n",
    "        for layer_weights, layer_bias, layer_activation_function in zip(self.weights, self.biases, self.activation_functions):\n",
    "\n",
    "            activation = propagate_forward(layer_weights, activation, layer_bias, layer_activation_function)\n",
    "            activation_history.append(activation)\n",
    "\n",
    "        # NOTE: The \"activation\" variable is changing as we go forward in the neural network.\n",
    "        y_pred = activation_history[-1].T\n",
    "        loss = MSE_loss(y_pred, y)\n",
    "\n",
    "        d_activations = d_MSE_loss(y_pred,y).T # NOTE: The final output can be \"seen as\" the final activations, thus the name.\n",
    "        \n",
    "        for i in range(len(self.weights))[::-1]:\n",
    "            # print(f\"Backward pass for layer {i}\")\n",
    "            current_activation = activation_history[i]\n",
    "            current_wights = self.weights[i]\n",
    "            current_bias = self.biases[i]\n",
    "            current_activation_function = self.activation_functions[i]\n",
    "\n",
    "            d_weights, d_activations, d_bias = propagate_backward(current_wights, current_activation, d_activations, current_bias, current_activation_function)\n",
    "            \n",
    "            self.weights[i] -= learning_rate * d_weights\n",
    "            self.biases[i] -= learning_rate * d_bias\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    # A function for the training of the network.\n",
    "    def train_net(self, x, y, batch_size=32, epochs=100, learning_rate=1e-4, use_validation_data=False, val_split=0.2):\n",
    "        \n",
    "        # TODO: Add a training loop where the weights and biases of the network is learnt over several epochs.\n",
    "\n",
    "        # TODO: Add support for mini batches. That is, in each epoch the data should be split into several\n",
    "        #       smaller subsets and the model should be trained on each of these subsets one at a time.\n",
    "\n",
    "        # TODO: Implement the use of validation data, that is, splitting the training data into training data and validation data.\n",
    "        #       The validation data should be used to stop the training when the model stops to generalise and starts to overfit.\n",
    "        #       This feature should be able to be turned on and off to test the difference.\n",
    "\n",
    "        # NOTE: Make use of previously implemented functions here.\n",
    "        num_samples = x.shape[0]\n",
    "        if use_validation_data:\n",
    "            split_idx = int(num_samples * (1 - val_split))\n",
    "            x_train, x_val = x[:split_idx], x[split_idx:]\n",
    "            y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "        else:\n",
    "            x_train, y_train = x, y\n",
    "\n",
    "        history = []\n",
    "        val_history = []\n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuffled = x_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for i in range(0, x_train_shuffled.shape[0], batch_size):\n",
    "                x_batch = x_train_shuffled[i:i+batch_size]\n",
    "                y_batch = y_train_shuffled[i:i+batch_size]\n",
    "                loss = self.adjust_weights(x_batch, y_batch, learning_rate)\n",
    "                epoch_loss += loss * x_batch.shape[0]\n",
    "            epoch_loss /= x_train_shuffled.shape[0]\n",
    "            history.append(epoch_loss)\n",
    "            \n",
    "            if use_validation_data:\n",
    "                y_val_pred = self.forward(x_val)\n",
    "                val_loss = MSE_loss(y_val_pred, y_val)\n",
    "                val_history.append(val_loss)\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.6f}, Validation Loss: {val_loss:.6f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.6f}\")\n",
    "        if use_validation_data:\n",
    "            return history, val_history\n",
    "        else:\n",
    "            return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0cc71",
   "metadata": {},
   "source": [
    "## Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb678ea9",
   "metadata": {},
   "source": [
    "### 7) Simple test\n",
    "\n",
    "In this a very simple test for you to use and toy around with before using the datasets.\n",
    "\n",
    "Make sure to test both the **adjust_weights** function and the **train_net** function. What is the difference between the two?\n",
    "\n",
    "Also, be sure to **plot the loss for each epoch** to see how the network training is progressing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You can change most things in this cell if you want to, we encurage it!\n",
    "\n",
    "n = 1000\n",
    "d = 4\n",
    "\n",
    "k = np.random.randint(0, 10, (d, 1))\n",
    "x = np.random.normal(0, 1, (n, d))\n",
    "y = np.dot(x, k) + 0.1 + np.random.normal(0, 0.01, (n, 1))\n",
    "\n",
    "nn = NeuralNet(d, 1, [18, 12])\n",
    "\n",
    "loss_1 = [nn.adjust_weights(x, y) for _ in range(1000)]\n",
    "\n",
    "\n",
    "# TODO: Use the train_net function to compare with the \"adjust_weights\" function.\n",
    "loss_2 = nn.train_net(x, y, batch_size=32, epochs=100, learning_rate=1e-4, use_validation_data=False, val_split=0.2)\n",
    "\n",
    "\n",
    "plt.plot(loss_1)\n",
    "plt.title(\"Loss 1 (adjust_weights)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_2)\n",
    "plt.title(\"Loss 2 (train_net)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d17c1",
   "metadata": {},
   "source": [
    "### Real test and preprocessing\n",
    "\n",
    "When using real data and neural networks, it is very important to scale the data between smaller values, usually between 0 and 1. This is because neural networks struggle with larger values as input compared to smaller values. \n",
    "\n",
    "To test this, we will use our first dataset and test with and without scaling.\n",
    "\n",
    "Similar as with assignment 2, we will use the scikit-learn library for this preprocessing: https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bddc1c7",
   "metadata": {},
   "source": [
    "### 8) Dataset 1: Wine - with and without scaling\n",
    "\n",
    "Wine dataset: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Train two neural network, one with scaling and one without. Are we able to see any difference in training results or loss over time?\n",
    "\n",
    "**Note:** Do not train for to many epochs (more than maybe 50-100). The network might \"learn\" anyway in the end, but you should still be able to see a difference when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6154fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data_wine = pd.read_csv(\"wine.csv\").to_numpy()\n",
    "\n",
    "# TODO: Set up the data and split it into train and test-sets.\n",
    "\n",
    "# TODO: Train and test your neural networks.\n",
    "# NOTE: Use the same train/test split for both neural network models!\n",
    "\n",
    "# TODO: Do the above at least 3 times\n",
    "# NOTE: Use loops here!\n",
    "\n",
    "# TODO: Plot the results with matplotlib (plt)\n",
    "# NOTE: One combined lineplot with the scaling and one without the scaling, 2 plots in total.\n",
    "# NOTE: Plot both the accuracy and the loss!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39ab63",
   "metadata": {},
   "source": [
    "### Real data and hyper-parameter tuning\n",
    "\n",
    "Now we are going to use real data, preprocess it, and do hyper-parameter tuning.\n",
    "\n",
    "Choose two hyper-parameters to tune to try and achive an even better result.\n",
    "\n",
    "**NOTE:** Changing the number of epochs should be part of the tuning, but it does not count towards the two hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a7c7a",
   "metadata": {},
   "source": [
    "### 9) Dataset 2: Mushroom\n",
    "\n",
    "Mushroom dataset: https://archive.ics.uci.edu/dataset/73/mushroom\n",
    "\n",
    "Note: This dataset has one feature with missing values. Remove this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "29a861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mushroom = pd.read_csv(\"mushroom.csv\").to_numpy()\n",
    "\n",
    "# TODO: Preprocess the data.\n",
    "\n",
    "# TODO: Split the data into train and test\n",
    "\n",
    "# TODO: Train a neural network on the data\n",
    "\n",
    "# TODO: Visualize the loss for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f363a",
   "metadata": {},
   "source": [
    "When hyper-parameter tuning, please write the parameters and network sizes you test here:\n",
    "\n",
    "* Parameter 1: \n",
    "* Parameter 2:\n",
    "\n",
    "* Neural network sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d9e9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyper-parameter tuning\n",
    "\n",
    "# TODO: Visualize the loss after hyper-parameter tuning for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy after hyper-parameter tuning for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8991d79",
   "metadata": {},
   "source": [
    "### 10) Dataset 3: Adult\n",
    "\n",
    "Adult dataset: https://archive.ics.uci.edu/dataset/2/adult\n",
    "\n",
    "**IMPORTANT NOTE:** This dataset is much larger than the previous two (48843 instances). If your code runs slow on your own computer, you may exclude parts of this dataset, but you must keep a minimum of 10000 datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80862998",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = pd.read(...) # TODO: Read the data.\n",
    "\n",
    "# TODO: Preprocess the data.\n",
    "\n",
    "# TODO: Split the data into train and test\n",
    "\n",
    "# TODO: Train a neural network on the data\n",
    "\n",
    "# TODO: Visualize the loss for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9bd1ff",
   "metadata": {},
   "source": [
    "When hyper-parameter tuning, please write the parameters and network sizes you test here:\n",
    "\n",
    "* Parameter 1: \n",
    "* Parameter 2:\n",
    "\n",
    "* Neural network sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5229f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyper-parameter tuning\n",
    "\n",
    "# TODO: Visualize the loss after hyper-parameter tuning for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy after hyper-parameter tuning for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df892ad",
   "metadata": {},
   "source": [
    "# Questions for examination:\n",
    "\n",
    "In addition to completing the assignment with all its tasks, you should also prepare to answer the following questions:\n",
    "\n",
    "1) Why would we want to use different activation functions?\n",
    "\n",
    "2) Why would we want to use different loss functions?\n",
    "\n",
    "3) Why are neural networks sensitive to large input values?\n",
    "\n",
    "4) What is the role of the bias? \n",
    "\n",
    "5) What is the purpose of hyper-parameter tuning?\n",
    "\n",
    "6) A small example neural network will be shown during the oral examination. You will be asked a few basic questions related to the number of weights, biases, inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cf459",
   "metadata": {},
   "source": [
    "# Finished!\n",
    "\n",
    "Was part of the setup incorrect? Did you spot any inconsistencies in the assignment? Could something improve?\n",
    "\n",
    "If so, please write them and send via email and send it to:\n",
    "\n",
    "* marcus.gullstrand@ju.se\n",
    "\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
